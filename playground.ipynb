{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Made Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "data": {
      "text/plain": "<module 'lib' (namespace)>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lib\n",
    "import numpy as np\n",
    "import spacy\n",
    "import tensorflow as tf\n",
    "\n",
    "from functional import seq\n",
    "from importlib import reload\n",
    "from joblib import Memory\n",
    "from lib.data import load_data as load_data_lib\n",
    "from lib.text_preprocessing import preprocess_tokens, lemmatize, remove_stopwords, to_lower, tokenize, words_only\n",
    "from lib.vocabulary import Vocabulary\n",
    "from tensorflow import keras\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "# tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "reload(lib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "mem = Memory('./data/cache', verbose=0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "preprocess_pipeline = [lemmatize, to_lower, words_only, remove_stopwords, tokenize]\n",
    "lang = spacy.load('de_dep_news_trf')\n",
    "\n",
    "@mem.cache\n",
    "def load_data():\n",
    "    res = load_data_lib()\n",
    "    return res\n",
    "\n",
    "data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "20070"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@mem.cache\n",
    "def preprocess_data():\n",
    "    return seq(tqdm(data['text'].to_numpy()))\\\n",
    "        .map(lambda text: preprocess_tokens(text, lang, preprocess_pipeline))\\\n",
    "        .to_list()\n",
    "\n",
    "samples = preprocess_data()\n",
    "len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "#preprocess_data.clear()\n",
    "#samples[:3]\n",
    "samples = samples[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 1009.97it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Vocabulary(name='default', size=1806, default_sample_length=2347)"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = seq(tqdm(samples))\\\n",
    "    .fold_left(lib.vocabulary.Vocabulary.builder(), lambda voc, sentence: voc.add_sample(sentence))\\\n",
    "    .build()\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "[('<EMPTY>', 0),\n ('the', 1),\n ('and', 2),\n ('we', 3),\n ('of', 4),\n ('to', 5),\n ('our', 6),\n ('i', 7),\n ('this', 8),\n ('for', 9)]"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocabulary.token2index.items())[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "#\n",
    "# Create model (traditional)\n",
    "#\n",
    "\n",
    "tm_model_in = keras.Input(shape=vocabulary.size, name='input')\n",
    "tm_embedding_in = keras.layers.Dense(50, activation=keras.activations.relu, name='embedding')\n",
    "tm_embedding_out = tm_embedding_in(tm_model_in)\n",
    "tm_softmax_in = keras.layers.Dense(vocabulary.size, activation=keras.activations.softmax, name='softmax')\n",
    "tm_softmax_out = tm_softmax_in(tm_embedding_out)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"word2vec_traditional\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 1806)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Dense)            (None, 50)                90350     \n",
      "_________________________________________________________________\n",
      "softmax (Dense)              (None, 1806)              92106     \n",
      "=================================================================\n",
      "Total params: 182,456\n",
      "Trainable params: 182,456\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# ...the model to learn the embedding (traditional)\n",
    "#\n",
    "\n",
    "tm_train_model = keras.Model(inputs=[tm_model_in], outputs=[tm_softmax_out], name='word2vec_traditional')\n",
    "tm_train_model.compile(\n",
    "    loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.RMSprop(),\n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "tm_train_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# ...the model to calculate an embedding vector the tokens (taditional)\n",
    "#\n",
    "\n",
    "tm_calc_embedding = keras.Model(inputs=[tm_train_model.input], outputs=[tm_train_model.get_layer('embedding').output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1, 50)        7293200     input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, 50, 1)        0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_7 (Reshape)             (None, 50, 1)        0           embedding[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dot_product (Dot)               (None, 1, 1)         0           reshape_6[0][0]                  \n",
      "                                                                 reshape_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_8 (Reshape)             (None, 1)            0           dot_product[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            2           reshape_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 7,293,202\n",
      "Trainable params: 7,293,202\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Create model\n",
    "#\n",
    "\n",
    "dim = 50\n",
    "\n",
    "word_input = keras.Input(shape=1)\n",
    "context_word_input = keras.Input(shape=1)\n",
    "\n",
    "embedding = keras.layers.Embedding(vocabulary.size, dim, input_length=1, name='embedding')\n",
    "word = keras.layers.Reshape((dim, 1))(embedding(word_input))\n",
    "context_word = keras.layers.Reshape((dim, 1))(embedding(context_word_input))\n",
    "similarity = keras.layers.Dot(1, normalize=True, name='similarity')([word, context_word])\n",
    "\n",
    "dot_product = keras.layers.Reshape((1,))(keras.layers.Dot(1, name='dot_product')([word, context_word]))\n",
    "output = keras.layers.Dense(1, activation=keras.activations.sigmoid, name='output')(dot_product)\n",
    "\n",
    "model = keras.Model(inputs=[word_input, context_word_input], outputs=output)\n",
    "model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.RMSprop())\n",
    "\n",
    "validation_model = keras.Model(inputs=[word_input, context_word_input], outputs=similarity)\n",
    "\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "#\n",
    "# Create a custom generator for generating Training samples from text.\n",
    "#\n",
    "\n",
    "from typing import List, Union\n",
    "import math\n",
    "\n",
    "class Word2VecDataGenerator(tf.keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, text_samples: Union[str, List[str]], voc: Vocabulary, window_size: int = 4, group_size: int = 100):\n",
    "        self._window_size = window_size\n",
    "        self._samples_idx = voc.samples_to_indices(text_samples, include_oov=False)\n",
    "        self._voc = voc\n",
    "        self._group_size = group_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        start_idx = index * self._group_size\n",
    "        end_idx = start_idx + self._group_size\n",
    "\n",
    "        words = np.zeros([0], dtype='int32')\n",
    "        context_words = np.zeros([0], dtype='int32')\n",
    "        labels = np.zeros([0], dtype='int8')\n",
    "\n",
    "        for i in range(start_idx, min(end_idx, self._samples_idx.shape[0])):\n",
    "            inputs_i, labels_i = self.__get_minibatch__(i)\n",
    "            words = np.concatenate([words, inputs_i[0]])\n",
    "            context_words = np.concatenate([context_words, inputs_i[1]])\n",
    "            labels = np.concatenate([labels, labels_i])\n",
    "\n",
    "        return [words, context_words], labels\n",
    "\n",
    "    def __get_minibatch__(self, index):\n",
    "        sample = self._samples_idx[index]\n",
    "        couples, labels = tf.keras.preprocessing.sequence.skipgrams(\n",
    "            sample, self._voc.size, window_size=self._window_size)\n",
    "\n",
    "        if len(couples) > 0:\n",
    "            words, context_words = zip(*couples)\n",
    "        else:\n",
    "            words, context_words, labels = [[],[],[]]\n",
    "\n",
    "        words = np.array(words, dtype='int32')\n",
    "        context_words = np.array(context_words, dtype='int32')\n",
    "        labels = np.array(labels, dtype='int8')\n",
    "\n",
    "        return [words, context_words], labels\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(self._samples_idx.shape[0] / self._group_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "class SimilarityLogger(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(\n",
    "            self, val_model: keras.Model, voc: Vocabulary, interval: int = 10, validation_size: int = 10, k: int = 5):\n",
    "        super().__init__()\n",
    "        self.vocabulary = voc\n",
    "        self.interval = interval\n",
    "        self.validation_size = validation_size\n",
    "        self.validation_model = val_model\n",
    "        self.k = k\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch == 0:\n",
    "            self.print_similarities()\n",
    "\n",
    "    def on_epoch_end(self, epoch: int, logs: dict = None) -> None:\n",
    "        if (epoch + 1) % self.interval != 0:\n",
    "            return\n",
    "        else:\n",
    "            self.print_similarities()\n",
    "\n",
    "\n",
    "    def print_similarities(self) -> None:\n",
    "        items = list(self.vocabulary.token2index.keys())[1:self.validation_size+1]\n",
    "        for item in items:\n",
    "            sim = self.get_similarity(item)\n",
    "            nearest = (-sim).argsort()[1:self.k + 1]\n",
    "\n",
    "            log = f\"Nearest neighbors to `{item}`:\"\n",
    "            for n in nearest:\n",
    "                n_word = self.vocabulary.index_to_token(n)\n",
    "                log = f\"{log} `{n_word}`\"\n",
    "\n",
    "            print(log)\n",
    "\n",
    "    def get_similarity(self, token: str):\n",
    "        sim = np.zeros(self.vocabulary.size)\n",
    "        word_array = np.zeros((1,1))\n",
    "        word_array[0,0] = self.vocabulary.token_to_index(token)\n",
    "        context_array = np.zeros((1,1))\n",
    "\n",
    "        for i in tqdm(range(1, self.vocabulary.size), desc=f\"`{token}` similarities\"):\n",
    "            context_array[0,0] = i\n",
    "            sim[i] = self.validation_model.predict_on_batch([word_array, context_array])\n",
    "\n",
    "        return sim\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "the similarities:   0%|          | 0/145863 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Functional' object has no attribute 'predict_function'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-57-950aa679edb6>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0msim_logger\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSimilarityLogger\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalidation_model\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvocabulary\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minterval\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m5\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m history = model.fit(\n\u001B[0m\u001B[1;32m      4\u001B[0m     \u001B[0mWord2VecDataGenerator\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msamples\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvocabulary\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0mepochs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m20\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Workspaces/thesis--news-preparation/env/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1088\u001B[0m       \u001B[0;32mfor\u001B[0m \u001B[0mepoch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0miterator\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menumerate_epochs\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1089\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreset_metrics\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1090\u001B[0;31m         \u001B[0mcallbacks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_epoch_begin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mepoch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1091\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcatch_stop_iteration\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1092\u001B[0m           \u001B[0;32mfor\u001B[0m \u001B[0mstep\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msteps\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Workspaces/thesis--news-preparation/env/lib/python3.8/site-packages/tensorflow/python/keras/callbacks.py\u001B[0m in \u001B[0;36mon_epoch_begin\u001B[0;34m(self, epoch, logs)\u001B[0m\n\u001B[1;32m    409\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mnumpy_logs\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# Only convert once.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    410\u001B[0m           \u001B[0mnumpy_logs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf_utils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_numpy_or_python_type\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlogs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 411\u001B[0;31m         \u001B[0mcallback\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_epoch_begin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mepoch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnumpy_logs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    412\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    413\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0mon_epoch_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mepoch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlogs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-56-e09db48e9627>\u001B[0m in \u001B[0;36mon_epoch_begin\u001B[0;34m(self, epoch, logs)\u001B[0m\n\u001B[1;32m     12\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mon_epoch_begin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mepoch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlogs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mepoch\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 14\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprint_similarities\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     15\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mon_epoch_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mepoch\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlogs\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mdict\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-56-e09db48e9627>\u001B[0m in \u001B[0;36mprint_similarities\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     24\u001B[0m         \u001B[0mitems\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvocabulary\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoken2index\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkeys\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalidation_size\u001B[0m\u001B[0;34m+\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     25\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mitem\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mitems\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 26\u001B[0;31m             \u001B[0msim\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_similarity\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     27\u001B[0m             \u001B[0mnearest\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0msim\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0margsort\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mk\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-56-e09db48e9627>\u001B[0m in \u001B[0;36mget_similarity\u001B[0;34m(self, token)\u001B[0m\n\u001B[1;32m     42\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtqdm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvocabulary\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msize\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdesc\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34mf\"{token} similarities\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     43\u001B[0m             \u001B[0mcontext_array\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 44\u001B[0;31m             \u001B[0msim\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalidation_model\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict_on_batch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mword_array\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcontext_array\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     45\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     46\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0msim\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Workspaces/thesis--news-preparation/env/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001B[0m in \u001B[0;36mpredict_on_batch\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m   1817\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdistribute_strategy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mscope\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1818\u001B[0m       \u001B[0miterator\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdata_adapter\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msingle_batch_iterator\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdistribute_strategy\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1819\u001B[0;31m       \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict_function\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmake_predict_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1820\u001B[0m       \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1821\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mtf_utils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_numpy_or_python_type\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Workspaces/thesis--news-preparation/env/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001B[0m in \u001B[0;36mmake_predict_function\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1452\u001B[0m       \u001B[0;31m`\u001B[0m\u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mIterator\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0;32mreturn\u001B[0m \u001B[0mthe\u001B[0m \u001B[0moutputs\u001B[0m \u001B[0mof\u001B[0m \u001B[0mthe\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0mModel\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1453\u001B[0m     \"\"\"\n\u001B[0;32m-> 1454\u001B[0;31m     \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict_function\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1455\u001B[0m       \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict_function\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1456\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'Functional' object has no attribute 'predict_function'"
     ]
    }
   ],
   "source": [
    "sim_logger = SimilarityLogger(validation_model, vocabulary, interval=5)\n",
    "\n",
    "history = model.fit(\n",
    "    Word2VecDataGenerator(samples, vocabulary),\n",
    "    epochs=20,\n",
    "    callbacks=[sim_logger])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}